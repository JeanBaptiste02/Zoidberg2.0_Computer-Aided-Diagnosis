{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ebbb8b5-52c8-41dd-9bc8-058096c16bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab117e2b-a461-4110-bb5d-15ef5fe04276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres\n",
    "img_size = 64\n",
    "batch_size = 32\n",
    "epochs_list = [7, 10, 13]\n",
    "data_dir1 = '../../../chest_xray/train/PNEUMONIA'\n",
    "data_dir2 = '../../../chest_xray/train/NORMAL'\n",
    "test_data_dir = '../../../chest_xray/test'\n",
    "validation_data_dir = '../../../chest_xray/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "236d1b52-f0d5-44df-950b-88f77dcb4988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement et prétraitement des données...\n",
      "loaded and preprocessed data\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour charger et prétraiter les données\n",
    "def load_and_preprocess_data(data_dir1, data_dir2, img_size, limit=None):\n",
    "    print(\"Chargement et prétraitement des données...\")\n",
    "    data = []\n",
    "    labels = ['PNEUMONIA', 'NORMAL']\n",
    "    for dir in [data_dir1, data_dir2]:\n",
    "        path = os.path.join(dir)\n",
    "        label = os.path.basename(dir)\n",
    "        class_num = labels.index(label)\n",
    "        for i, img in enumerate(os.listdir(path)):\n",
    "            if limit and i >= limit:\n",
    "                break\n",
    "            try:\n",
    "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                if img_arr is None:\n",
    "                    continue\n",
    "                resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
    "                data.append([resized_arr, class_num])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    data = np.array(data, dtype=object)\n",
    "    print(\"loaded and preprocessed data\")\n",
    "    return data\n",
    "\n",
    "# Appel de la fonction load_and_preprocess_data\n",
    "train_data = load_and_preprocess_data(data_dir1, data_dir2, img_size, limit=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c440168-4cd9-4f7a-b2c3-53a8a5c773f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Séparation des caractéristiques et des labels...\n",
      "Séparation terminée.\n",
      "Prétraitement des données...\n",
      "Séparation des caractéristiques et des labels...\n",
      "Séparation terminée.\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour séparer les caractéristiques et les labels\n",
    "def separate_features_labels(data):\n",
    "    print(\"Séparation des caractéristiques et des labels...\")\n",
    "    features = []\n",
    "    labels = []\n",
    "    for img, label in data:\n",
    "        features.append(img)\n",
    "        labels.append(label)\n",
    "    print(\"Séparation terminée.\")\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Appel de la fonction separate_features_labels\n",
    "x_data, y_data = separate_features_labels(train_data)\n",
    "\n",
    "# Fonction pour prétraiter les données (normalisation)\n",
    "def preprocess_data(data):\n",
    "    print(\"Prétraitement des données...\")\n",
    "    processed_data = []\n",
    "    for img, label in data:\n",
    "        img = cv2.normalize(img.astype('float32'), None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "        img = np.repeat(img, 3, axis=-1)  # Convertir en 3 canaux\n",
    "        processed_data.append([img, label])\n",
    "    return np.array(processed_data, dtype=object)\n",
    "\n",
    "# Appel de la fonction preprocess_data\n",
    "train_data = preprocess_data(train_data)\n",
    "x_data, y_data = separate_features_labels(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "189b6818-18bf-4682-883b-a808976f7b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création des générateurs de données...\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour créer le générateur d'images augmentées\n",
    "def create_data_generators(x_data, y_data, img_size, batch_size):\n",
    "    print(\"Création des générateurs de données...\")\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    datagen.fit(x_data)\n",
    "    return datagen\n",
    "\n",
    "# Appel de la fonction create_data_generators\n",
    "datagen = create_data_generators(x_data, y_data, img_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "821d4074-1c9c-4c69-8db8-1d4d9be36b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création du modèle CNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikne\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle CNN créé et compilé.\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour créer le modèle CNN\n",
    "def create_model():\n",
    "    print(\"Création du modèle CNN...\")\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', 'AUC'])\n",
    "    print(\"Modèle CNN créé et compilé.\")\n",
    "    return model\n",
    "\n",
    "# Appel de la fonction create_model\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a20aae9-17ee-4ef3-ad7d-c7d74f2ad9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimisation du nombre d'époques...\n",
      "\n",
      "Testing 7 epochs...\n",
      "Évaluation du modèle avec 7 époques...\n",
      "Création du modèle CNN...\n",
      "Modèle CNN créé et compilé.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikne\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing 10 epochs...\n",
      "Évaluation du modèle avec 10 époques...\n",
      "Création du modèle CNN...\n",
      "Modèle CNN créé et compilé.\n",
      "\n",
      "Testing 13 epochs...\n",
      "Évaluation du modèle avec 13 époques...\n",
      "Création du modèle CNN...\n",
      "Modèle CNN créé et compilé.\n",
      "Le meilleur nombre d'époques est 13 avec une précision de validation de 0.81.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fonction pour évaluer le modèle avec un certain nombre d'époques\n",
    "def evaluate_model_with_epochs(epochs, x_train, y_train, x_val, y_val, datagen):\n",
    "    print(f\"Évaluation du modèle avec {epochs} époques...\")\n",
    "    model = create_model()\n",
    "    history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        callbacks=[ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.0001),\n",
    "                                   EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "                        verbose=0)\n",
    "    return history\n",
    "\n",
    "# Fonction pour faire une validation croisée et choisir le meilleur nombre d'époques\n",
    "def optimize_epochs(x_data, y_data, epochs_list, batch_size):\n",
    "    print(\"Optimisation du nombre d'époques...\")\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "    best_accuracy = 0\n",
    "    best_epochs = 0\n",
    "    best_history = None\n",
    "\n",
    "    for epochs in epochs_list:\n",
    "        print(f\"\\nTesting {epochs} epochs...\")\n",
    "        history = evaluate_model_with_epochs(epochs, x_train, y_train, x_val, y_val, datagen)\n",
    "        val_accuracy = history.history['val_accuracy'][-1]\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_epochs = epochs\n",
    "            best_history = history\n",
    "\n",
    "    print(f\"Le meilleur nombre d'époques est {best_epochs} avec une précision de validation de {best_accuracy:.2f}.\")\n",
    "    return best_epochs, best_history\n",
    "\n",
    "# Appel de la fonction optimize_epochs\n",
    "best_epochs, best_history = optimize_epochs(x_data, y_data, epochs_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1601332c-5d73-45c3-ae27-8193c4545eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour évaluer les performances du modèle\n",
    "def evaluate_final_model(model, x_test, y_test):\n",
    "    print(\"Évaluation du modèle final...\")\n",
    "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Précision de test : {scores[1]*100:.2f}%\")\n",
    "    print(f\"Perte de test : {scores[0]:.4f}\")\n",
    "\n",
    "    # Matrice de confusion\n",
    "    y_pred = (model.predict(x_test) > 0.5).astype(\"int32\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['PNEUMONIA', 'NORMAL'], yticklabels=['PNEUMONIA', 'NORMAL'])\n",
    "    plt.ylabel('Réel')\n",
    "    plt.xlabel('Prédiction')\n",
    "    plt.title('Matrice de Confusion')\n",
    "    plt.show()\n",
    "\n",
    "    # Rapport de classification\n",
    "    cr = classification_report(y_test, y_pred, target_names=['PNEUMONIA', 'NORMAL'])\n",
    "    print(\"Rapport de Classification:\\n\", cr)\n",
    "\n",
    "    # Courbe ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='Courbe ROC (aire = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Taux de Faux Positifs')\n",
    "    plt.ylabel('Taux de Vrais Positifs')\n",
    "    plt.title('Caractéristique de Performance du Modèle')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "854255b8-d953-43e3-82c5-85be75f2f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour afficher les courbes d'apprentissage\n",
    "def plot_learning_curves(history):\n",
    "    print(\"Affichage des courbes d'apprentissage...\")\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Courbe de Précision')\n",
    "    plt.xlabel('Époques')\n",
    "    plt.ylabel('Précision')\n",
    "    plt.legend(['Entraînement', 'Validation'])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Courbe de Perte')\n",
    "    plt.xlabel('Époques')\n",
    "    plt.ylabel('Perte')\n",
    "    plt.legend(['Entraînement', 'Validation'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "973d4d92-3050-414f-aab9-9b9e0a059f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle final avec 13 époques...\n",
      "Création du modèle CNN...\n",
      "Modèle CNN créé et compilé.\n",
      "Epoch 1/13\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 156ms/step - AUC: 0.4645 - accuracy: 0.4750 - loss: 0.7172 - val_AUC: 0.7219 - val_accuracy: 0.6300 - val_loss: 0.6913 - learning_rate: 0.0010\n",
      "Epoch 2/13\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 121ms/step - AUC: 0.5540 - accuracy: 0.5321 - loss: 0.6905 - val_AUC: 0.7302 - val_accuracy: 0.5200 - val_loss: 0.6808 - learning_rate: 0.0010\n",
      "Epoch 3/13\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - AUC: 0.6393 - accuracy: 0.5677 - loss: 0.6835 - val_AUC: 0.7499 - val_accuracy: 0.5200 - val_loss: 0.6719 - learning_rate: 0.0010\n",
      "Epoch 4/13\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step - AUC: 0.6611 - accuracy: 0.6014 - loss: 0.6611 - val_AUC: 0.6957 - val_accuracy: 0.5550 - val_loss: 0.6801 - learning_rate: 0.0010\n",
      "Epoch 5/13\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 121ms/step - AUC: 0.6761 - accuracy: 0.6140 - loss: 0.6504 - val_AUC: 0.7361 - val_accuracy: 0.5250 - val_loss: 0.6884 - learning_rate: 0.0010\n",
      "Epoch 6/13\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - AUC: 0.7204 - accuracy: 0.6711 - loss: 0.6204 - val_AUC: 0.8205 - val_accuracy: 0.7400 - val_loss: 0.5657 - learning_rate: 0.0010\n",
      "Epoch 7/13\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - AUC: 0.7384 - accuracy: 0.6704 - loss: 0.6041 - val_AUC: 0.7679 - val_accuracy: 0.7250 - val_loss: 0.5618 - learning_rate: 0.0010\n",
      "Epoch 8/13\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step - AUC: 0.7184 - accuracy: 0.6713 - loss: 0.6180 - val_AUC: 0.8276 - val_accuracy: 0.7500 - val_loss: 0.4987 - learning_rate: 0.0010\n",
      "Epoch 9/13\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 126ms/step - AUC: 0.7622 - accuracy: 0.7089 - loss: 0.5771 - val_AUC: 0.8030 - val_accuracy: 0.7350 - val_loss: 0.5345 - learning_rate: 0.0010\n",
      "Epoch 10/13\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - AUC: 0.7960 - accuracy: 0.7188 - loss: 0.5402 - val_AUC: 0.8417 - val_accuracy: 0.7450 - val_loss: 0.5133 - learning_rate: 0.0010\n",
      "Epoch 11/13\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step - AUC: 0.7650 - accuracy: 0.6926 - loss: 0.5691 - val_AUC: 0.9110 - val_accuracy: 0.5950 - val_loss: 0.6090 - learning_rate: 0.0010\n",
      "Epoch 12/13\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 123ms/step - AUC: 0.8155 - accuracy: 0.7033 - loss: 0.5454 - val_AUC: 0.9045 - val_accuracy: 0.7800 - val_loss: 0.4258 - learning_rate: 5.0000e-04\n",
      "Epoch 13/13\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 126ms/step - AUC: 0.8234 - accuracy: 0.7337 - loss: 0.5236 - val_AUC: 0.9442 - val_accuracy: 0.8100 - val_loss: 0.3987 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour l'entraînement final du modèle\n",
    "def train_final_model(x_train, y_train, x_val, y_val, epochs):\n",
    "    print(f\"Entraînement du modèle final avec {epochs} époques...\")\n",
    "    model = create_model()\n",
    "    history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        callbacks=[ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.0001),\n",
    "                                   EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)])\n",
    "    return model, history\n",
    "\n",
    "# Appel de la fonction train_final_model\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "final_model, final_history = train_final_model(x_train, y_train, x_val, y_val, best_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99fe2267-2868-4582-873d-2d3c2665f3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enregistrement du modèle sous 'final_cnn_model.h5'...\n",
      "Modèle sauvegardé sous 'final_cnn_model.h5'.\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour sauvegarder le modèle\n",
    "def save_model(model, filename):\n",
    "    print(f\"Enregistrement du modèle sous '{filename}'...\")\n",
    "    model.save(filename)\n",
    "    print(f\"Modèle sauvegardé sous '{filename}'.\")\n",
    "\n",
    "# Appel de la fonction save_model\n",
    "save_model(final_model, 'final_cnn_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
