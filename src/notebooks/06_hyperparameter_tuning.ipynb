{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 17m 26s]\n",
      "val_accuracy: 0.90625\n",
      "\n",
      "Best val_accuracy So Far: 0.90625\n",
      "Total elapsed time: 00h 17m 26s\n",
      "\n",
      "Search: Running Trial #2\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "48                |32                |conv_1_filter\n",
      "5                 |5                 |conv_1_kernel\n",
      "48                |64                |conv_2_filter\n",
      "5                 |3                 |conv_2_kernel\n",
      "48                |96                |dense_1_units\n",
      "0.3               |0.2               |dropout_rate\n",
      "0.001             |0.001             |learning_rate\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m122/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 484ms/step - accuracy: 0.7345 - loss: 0.5871"
     ]
    }
   ],
   "source": [
    "# Install keras-tuner if not already installed\n",
    "import sys\n",
    "!{sys.executable} -m pip install keras-tuner\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Define constants\n",
    "labels = ['PNEUMONIA', 'NORMAL']\n",
    "img_size = 150\n",
    "\n",
    "# Function to get data from directory\n",
    "def get_data(data_dir):\n",
    "    data = [] \n",
    "    for label in labels: \n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                if img_arr is not None:  # Ensure the image is read correctly\n",
    "                    resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
    "                    data.append([resized_arr, class_num])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return np.array(data, dtype=object)\n",
    "\n",
    "# Load data\n",
    "train = get_data('../../chest_xray/train')\n",
    "val = get_data('../../chest_xray/val')\n",
    "\n",
    "# Separate features and labels\n",
    "x_train, y_train = [], []\n",
    "x_val, y_val = [], []\n",
    "\n",
    "for feature, label in train:\n",
    "    x_train.append(feature)\n",
    "    y_train.append(label)\n",
    "    \n",
    "for feature, label in val:\n",
    "    x_val.append(feature)\n",
    "    y_val.append(label)\n",
    "\n",
    "# Normalize the data\n",
    "x_train = np.array(x_train) / 255.0\n",
    "x_val = np.array(x_val) / 255.0\n",
    "\n",
    "# Reshape data for deep learning\n",
    "x_train = x_train.reshape(-1, img_size, img_size, 1)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_val = x_val.reshape(-1, img_size, img_size, 1)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# Hypermodel function\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=16), kernel_size=hp.Choice('conv_1_kernel', values=[3,5]), activation='relu', input_shape=(img_size, img_size, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=hp.Int('conv_2_filter', min_value=32, max_value=64, step=16), kernel_size=hp.Choice('conv_2_kernel', values=[3,5]), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=hp.Int('dense_1_units', min_value=32, max_value=128, step=16), activation='relu'))\n",
    "    model.add(Dropout(rate=hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Keras Tuner with limited trials and epochs\n",
    "tuner = kt.RandomSearch(build_model,\n",
    "                        objective='val_accuracy',\n",
    "                        max_trials=10,  # Limit the number of trials\n",
    "                        executions_per_trial=2,  # Average results over multiple executions\n",
    "                        directory='hyperparam_tuning',\n",
    "                        project_name='chest_xray_tuning')\n",
    "\n",
    "# Callback for reducing learning rate\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=2, min_lr=1e-5)\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Hyperparameter search\n",
    "tuner.search(datagen.flow(x_train, y_train, batch_size=32),\n",
    "             epochs=10,  # Limit the number of epochs\n",
    "             validation_data=(x_val, y_val),\n",
    "             callbacks=[reduce_lr])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of filters in the first convolutional layer is {best_hps.get('conv_1_filter')},\n",
    "the optimal kernel size for the first convolutional layer is {best_hps.get('conv_1_kernel')},\n",
    "the optimal number of filters in the second convolutional layer is {best_hps.get('conv_2_filter')},\n",
    "the optimal kernel size for the second convolutional layer is {best_hps.get('conv_2_kernel')},\n",
    "the optimal number of units in the dense layer is {best_hps.get('dense_1_units')},\n",
    "the optimal dropout rate is {best_hps.get('dropout_rate')},\n",
    "and the optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
    "                    epochs=12,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[reduce_lr])\n",
    "\n",
    "# Save the model and training history\n",
    "model.save(\"cnn_tuned_model.h5\")\n",
    "np.save('tuned_training_history.npy', history.history)\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
